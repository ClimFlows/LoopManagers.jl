var documenterSearchIndex = {"docs":
[{"location":"#LoopManagers","page":"Home","title":"LoopManagers","text":"Documentation for LoopManagers.\n\n","category":"section"},{"location":"#LoopManagers.LoopManagers","page":"Home","title":"LoopManagers.LoopManagers","text":"Module LoopManagers provides computing managers to pass to functions using the performance portability module ManagedLoops. It implements the API functions defined by ManagedLoops for the provided managers. Currently supported are SIMD and/or multithreaded execution on the CPU. Offloading to GPU via CUDA and oneAPI is experimental.\n\nAdditional iteration/offloading strategies (e.g. cache-friendly iteration) can be implemented by defining new manager types and implementing specialized versions of ManagedLoops.offload.\n\n\n\n\n\n","category":"module"},{"location":"#LoopManagers.MainThread","page":"Home","title":"LoopManagers.MainThread","text":"manager = MainThread(cpu_manager=PlainCPU(), nthreads=Threads.nthreads())\n\nReturns a multithread manager derived from cpu_manager, initially in sequential mode. In this mode, manager behaves exactly like cpu_manager. When manager is passed to ManagedLoops.parallel, nthreads threads are spawn. The manager passed to threads works in parallel mode. In this mode, manager behaves like cpu_manager, except that the outer loop is distributed among threads. Furthermore ManagedLoops.barrier and ManagedLoops.share allow synchronisation and data-sharing across threads.\n\nmain_mgr = MainThread()\nLoopManagers.parallel(main_mgr) do thread_mgr\n    x = LoopManagers.share(thread_mgr) do master_mgr\n        randn()\n    end\n    println(\"Thread $(Threads.threadid()) has drawn $x.\")\nend\n\n\n\n\n\n","category":"type"},{"location":"#LoopManagers.MultiThread","page":"Home","title":"LoopManagers.MultiThread","text":"manager = MultiThread(b=PlainCPU(), nt=Threads.nthreads())\n\nReturns a multithread manager derived from cpu_manager, with a fork-join pattern. When manager is passed to ManagedLoops.offload, manager.nthreads threads are spawn (fork). They each work on a subset of indices. Progress continues only after all threads have finished (join), so that barrier is not needed between two uses of offload and does nothing.\n\ntip: Tip\nIt is highly recommended to pin the Julia threads to specific cores. The simplest way is probably to set JULIA_EXCLUSIVE=1 before launching Julia. See also Julia Discourse\n\n\n\n\n\n","category":"type"},{"location":"#LoopManagers.PlainCPU","page":"Home","title":"LoopManagers.PlainCPU","text":"manager = PlainCPU()\n\nManager for sequential execution on the CPU. LLVM will try to vectorize loops marked with @simd. This works mostly for simple loops and arithmetic computations. For Julia-side vectorization, especially of mathematical functions, see `VectorizedCPU'.\n\n\n\n\n\n","category":"type"},{"location":"#LoopManagers.SingleCPU","page":"Home","title":"LoopManagers.SingleCPU","text":"abstract type SingleCPU<:HostManager end\n\nParent type for manager executing on a single core. Derived types should specialize distribute[@ref] or offload_single[@ref] and leave offload as it is.\n\n\n\n\n\n","category":"type"},{"location":"#LoopManagers.VectorizedCPU","page":"Home","title":"LoopManagers.VectorizedCPU","text":"manager = VectorizedCPU()\n\nReturns a manager for executing loops with optional explicit SIMD vectorization. Only inner loops marked with @vec will use explicit vectorization. If this causes errors, use @simd instead of @vec. Vectorization of loops marked with @simd is left to the Julia/LLVM compiler, as with PlainCPU.\n\nnote: Note\nManagedLoops.no_simd(::VectorizedCPU) returns a PlainCPU.\n\n\n\n\n\n","category":"type"},{"location":"#LoopManagers.KernelAbstractions_GPU","page":"Home","title":"LoopManagers.KernelAbstractions_GPU","text":"gpu = KernelAbstractions_GPU(gpu::KernelAbstractions.GPU, ArrayType)\n# examples\ngpu = KernelAbstractions_GPU(CUDABackend(), CuArray)\ngpu = KernelAbstractions_GPU(ROCBackend(), ROCArray)\ngpu = KernelAbstractions_GPU(oneBackend(), oneArray)\n\nReturns a manager that offloads computations to a KernelAbstractions GPU backend. The returned manager will call ArrayType(data) when it needs to transfer data to the device.\n\nnote: Note\nWhile KA_GPU is always available, implementations of [offload] are available only if the module KernelAbstractions is loaded by the main program or its dependencies.\n\n\n\n\n\n","category":"function"},{"location":"#LoopManagers.distribute-Tuple{Any, VectorizedCPU, Any, Any}","page":"Home","title":"LoopManagers.distribute","text":"Divide work among vectorized CPU threads.\n\n\n\n\n\n","category":"method"}]
}
